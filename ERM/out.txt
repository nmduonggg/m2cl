Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: ERM
	alpha: None
	batch_size: 8
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: ERM
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: None
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: None
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.4346289753  0.4911660777  0.4094117647  0.4256120527  0.3697638995  0.3734756098  0.2606442058  0.2474074074  0.0000000000  1.9539786577  0.6192207336  0             1.2612032890 
0.9964664311  0.9964664311  0.7289411765  0.7175141243  0.7829398324  0.7408536585  0.7300999630  0.7451851852  2.1201413428  0.5286160829  0.7135796547  300           0.1294891453 
0.9955830389  0.9929328622  0.7609411765  0.6949152542  0.8099771516  0.7378048780  0.7241762310  0.7022222222  4.2402826855  0.4154697707  0.7135796547  600           0.1390072242 
1.0000000000  0.9964664311  0.7816470588  0.7476459510  0.8099771516  0.7362804878  0.7475009256  0.7407407407  6.3604240283  0.3653064449  0.7135796547  900           0.1335492094 
0.9955830389  0.9893992933  0.7792941176  0.7118644068  0.8274942879  0.7347560976  0.7500925583  0.7496296296  8.4805653710  0.3443445311  0.7135796547  1200          0.1368836443 
0.9991166078  1.0000000000  0.8235294118  0.7645951036  0.8469154608  0.7439024390  0.7282487967  0.7111111111  10.600706713  0.2854883048  0.7135796547  1500          0.1341924318 
1.0000000000  0.9964664311  0.8230588235  0.6836158192  0.8945163747  0.7621951220  0.7278785635  0.7066666667  12.720848056  0.2730589929  0.7135796547  1800          0.1359443394 
1.0000000000  0.9964664311  0.8687058824  0.7664783427  0.9013709063  0.7530487805  0.6727138097  0.6651851852  14.840989399  0.2549992018  0.7135796547  2100          0.1196928970 
1.0000000000  0.9929328622  0.8922352941  0.7476459510  0.9146991622  0.7439024390  0.6775268419  0.6651851852  16.961130742  0.2104192794  0.7135796547  2400          0.1230361859 
1.0000000000  0.9964664311  0.8042352941  0.7099811676  0.8187357197  0.6890243902  0.5864494632  0.5940740741  19.081272084  0.1823036326  0.7135796547  2700          0.1413586307 
1.0000000000  1.0000000000  0.9011764706  0.7250470810  0.9493526276  0.7637195122  0.7115883006  0.7096296296  21.201413427  0.1679074268  0.7135796547  3000          0.1347332191 
1.0000000000  1.0000000000  0.9124705882  0.7156308851  0.9341203351  0.7225609756  0.7178822658  0.7155555556  23.321554770  0.1418387084  0.7135796547  3300          0.1382848056 
0.9991166078  0.9893992933  0.8244705882  0.6440677966  0.9482102056  0.7515243902  0.7238059978  0.7007407407  25.441696113  0.1436616285  0.7135796547  3600          0.1324401196 
1.0000000000  1.0000000000  0.9472941176  0.7269303202  0.9661081493  0.7804878049  0.6941873380  0.6770370370  27.561837455  0.1041210748  0.7135796547  3900          0.1288760916 
1.0000000000  1.0000000000  0.9581176471  0.7325800377  0.9718202589  0.7545731707  0.6819696409  0.6785185185  29.681978798  0.0933339152  0.7135796547  4200          0.1101811600 
0.9973498233  0.9893992933  0.9642352941  0.7551789077  0.9756283321  0.7393292683  0.7249166975  0.7200000000  31.802120141  0.1031870815  0.7135796547  4500          0.1161932437 
0.9991166078  0.9964664311  0.9632941176  0.7269303202  0.9737242955  0.7667682927  0.6634579785  0.6859259259  33.922261484  0.0819080916  0.7135796547  4800          0.1117386572 
0.9982332155  1.0000000000  0.9618823529  0.6930320151  0.9722010663  0.7393292683  0.5994076268  0.5911111111  36.042402826  0.0745526734  0.7135796547  5100          0.1074472086 
1.0000000000  1.0000000000  0.9675294118  0.7419962335  0.9790555979  0.7301829268  0.6993706035  0.6977777778  38.162544169  0.0815888163  0.7135796547  5400          0.1115342848 
1.0000000000  0.9964664311  0.9675294118  0.7344632768  0.9619192688  0.7591463415  0.6556830803  0.6577777778  40.282685512  0.0548463015  0.7135796547  5700          0.1076910559 
1.0000000000  0.9964664311  0.9708235294  0.7344632768  0.9737242955  0.7149390244  0.6901147723  0.6711111111  42.402826855  0.0694219307  0.7135796547  6000          0.1120268782 
1.0000000000  1.0000000000  0.9755294118  0.7156308851  0.9760091394  0.7454268293  0.6449463162  0.6533333333  44.522968197  0.0540668101  0.7135796547  6300          0.0996210059 
1.0000000000  0.9893992933  0.9463529412  0.7344632768  0.9623000762  0.7210365854  0.6830803406  0.6903703704  46.643109540  0.0505287011  0.7135796547  6600          0.1098106933 
1.0000000000  0.9964664311  0.9717647059  0.7419962335  0.9801980198  0.7210365854  0.6967789708  0.6814814815  48.763250883  0.0589120390  0.7135796547  6900          0.1094330875 
0.9991166078  0.9964664311  0.9764705882  0.7231638418  0.9828636710  0.7378048780  0.7012217697  0.6933333333  50.883392226  0.0418479931  0.7135796547  7200          0.1078932722 
0.9991166078  0.9964664311  0.9863529412  0.6911487759  0.9843869002  0.7393292683  0.6467974824  0.6296296296  53.003533568  0.0534025645  0.7135796547  7500          0.1037577510 
1.0000000000  0.9964664311  0.9807058824  0.7363465160  0.9862909368  0.7454268293  0.7045538689  0.6874074074  55.123674911  0.0492064195  0.7135796547  7800          0.1126102646 
0.9991166078  0.9929328622  0.9792941176  0.7514124294  0.9771515613  0.7317073171  0.6986301370  0.6992592593  57.243816254  0.0458957990  0.7135796547  8100          0.1126443760 
1.0000000000  0.9964664311  0.9769411765  0.7231638418  0.9897182026  0.7560975610  0.7123287671  0.7037037037  59.363957597  0.0392699116  0.7135796547  8400          0.1107875649 
1.0000000000  1.0000000000  0.9863529412  0.7193973635  0.9840060929  0.7362804878  0.6434653832  0.6281481481  61.484098939  0.0534017682  0.7135796547  8700          0.1132688340 
1.0000000000  0.9964664311  0.9684705882  0.7193973635  0.9687738005  0.7271341463  0.6797482414  0.6592592593  63.604240282  0.0411089471  0.7135796547  9000          0.1133050497 
1.0000000000  1.0000000000  0.9811764706  0.7118644068  0.9912414318  0.7393292683  0.7012217697  0.6888888889  65.724381625  0.0394901779  0.7135796547  9300          0.1068270564 
1.0000000000  1.0000000000  0.9675294118  0.7382297552  0.9687738005  0.7317073171  0.6693817105  0.6711111111  67.844522968  0.0351910027  0.7135796547  9600          0.1080827149 
1.0000000000  1.0000000000  0.9891764706  0.7156308851  0.9813404417  0.7393292683  0.6664198445  0.6429629630  69.964664311  0.0546705250  0.7135796547  9900          0.1082965962 
1.0000000000  1.0000000000  0.9863529412  0.7250470810  0.9893373953  0.7393292683  0.7004813032  0.6844444444  70.664310954  0.0248934732  0.7135796547  9999          0.0842533232 
========== Best Results ==========
1.0000000000  1.0000000000  0.9891764706  0.7664783427  0.9912414318  0.7804878049  0.7500925583  0.7496296296  70.664310954  1.9539786577  0.7135796547  9999          1.2612032890 
Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: ERM
	alpha: None
	batch_size: 8
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: ERM
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: None
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: None
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1696156193  0.1809290954  0.1673773987  0.1388888889  0.2065868263  0.2005988024  0.1809796438  0.1821656051  0.0000000000  2.0157012939  0.6192364693  0             0.9635336399 
0.9029896278  0.8630806846  0.8987206823  0.8675213675  0.9633233533  0.9191616766  0.6380407125  0.6636942675  1.7964071856  0.4515905694  0.7110009193  300           0.0742997074 
0.8999389872  0.8386308068  0.9307036247  0.8910256410  0.9745508982  0.9251497006  0.6682569975  0.6828025478  3.5928143713  0.1803814571  0.7110009193  600           0.0740870182 
0.9402074436  0.8753056235  0.9323027719  0.8824786325  0.9850299401  0.9341317365  0.6269083969  0.6407643312  5.3892215569  0.1445178730  0.7110009193  900           0.0744129213 
0.9682733374  0.9144254279  0.9738805970  0.9252136752  0.9970059880  0.9640718563  0.7267811705  0.7477707006  7.1856287425  0.1038106528  0.7110009193  1200          0.0757001193 
0.9621720561  0.8924205379  0.9866737740  0.9294871795  0.9955089820  0.9550898204  0.7395038168  0.7605095541  8.9820359281  0.0847702038  0.7110009193  1500          0.0754019348 
0.9731543624  0.9070904645  0.9722814499  0.9252136752  0.9932634731  0.9580838323  0.7331424936  0.7363057325  10.778443113  0.0790464333  0.7110009193  1800          0.0720160158 
0.9682733374  0.8826405868  0.9893390192  0.9316239316  0.9925149701  0.9640718563  0.7185114504  0.7363057325  12.574850299  0.0597337530  0.7110009193  2100          0.0722085333 
0.9719341062  0.9070904645  0.9850746269  0.9380341880  0.9962574850  0.9371257485  0.7204198473  0.7197452229  14.371257485  0.0486975454  0.7110009193  2400          0.0724918914 
0.9725442343  0.9095354523  0.9808102345  0.9294871795  0.9962574850  0.9431137725  0.7121501272  0.7184713376  16.167664670  0.0553227126  0.7110009193  2700          0.0750623957 
0.9841366687  0.9168704156  0.9797441365  0.9230769231  0.9970059880  0.9580838323  0.6797073791  0.6840764331  17.964071856  0.0559774750  0.7110009193  3000          0.0762640587 
0.9896278218  0.9119804401  0.9893390192  0.9123931624  0.9970059880  0.9580838323  0.7255089059  0.7095541401  19.760479041  0.0448524843  0.7110009193  3300          0.0776031645 
0.9890176937  0.9119804401  0.9909381663  0.9358974359  0.9962574850  0.9461077844  0.7185114504  0.7273885350  21.556886227  0.0416225070  0.7110009193  3600          0.0753749379 
0.9902379500  0.8899755501  0.9786780384  0.9123931624  0.9872754491  0.9431137725  0.7162849873  0.7019108280  23.353293413  0.0334703403  0.7110009193  3900          0.0776782870 
0.9938987187  0.9095354523  0.9957356077  0.9316239316  0.9977544910  0.9491017964  0.7763994911  0.7859872611  25.149700598  0.0321356711  0.7110009193  4200          0.0805580131 
0.9871873093  0.8997555012  0.9866737740  0.9294871795  1.0000000000  0.9580838323  0.6825699746  0.6878980892  26.946107784  0.0340015737  0.7110009193  4500          0.0816855240 
0.9908480781  0.9290953545  0.9914712154  0.9273504274  1.0000000000  0.9461077844  0.7633587786  0.7707006369  28.742514970  0.0425191249  0.7110009193  4800          0.0836912735 
0.9896278218  0.9046454768  0.9824093817  0.9188034188  0.9992514970  0.9550898204  0.7738549618  0.7732484076  30.538922155  0.0272761654  0.7110009193  5100          0.0825538174 
0.9884075656  0.8973105134  0.9882729211  0.9230769231  0.9985029940  0.9520958084  0.7274173028  0.7363057325  32.335329341  0.0233855922  0.7110009193  5400          0.0820848711 
0.9572910311  0.8581907090  0.9914712154  0.9252136752  0.9925149701  0.9491017964  0.7639949109  0.7681528662  34.131736526  0.0324579676  0.7110009193  5700          0.0824741356 
0.9877974375  0.9046454768  0.9808102345  0.8931623932  0.9962574850  0.9491017964  0.7484096692  0.7388535032  35.928143712  0.0301541656  0.7110009193  6000          0.0827206810 
0.9798657718  0.9022004890  0.9824093817  0.9401709402  0.9947604790  0.9401197605  0.7595419847  0.7605095541  37.724550898  0.0288395759  0.7110009193  6300          0.0818836451 
0.9810860281  0.8753056235  0.9856076759  0.9102564103  0.9970059880  0.9401197605  0.6097328244  0.6000000000  39.520958083  0.0198893252  0.7110009193  6600          0.0825135589 
0.9945088469  0.9339853301  0.9925373134  0.9358974359  0.9992514970  0.9491017964  0.7496819338  0.7503184713  41.317365269  0.0278459545  0.7110009193  6900          0.0835321848 
0.9920683344  0.9437652812  0.9957356077  0.9423076923  1.0000000000  0.9760479042  0.7646310433  0.7656050955  43.113772455  0.0315081105  0.7110009193  7200          0.0851380905 
0.9926784625  0.9217603912  0.9952025586  0.9252136752  0.9970059880  0.9461077844  0.7449109415  0.7541401274  44.910179640  0.0193041700  0.7110009193  7500          0.0857021141 
0.9938987187  0.9193154034  0.9930703625  0.9529914530  0.9985029940  0.9550898204  0.7595419847  0.7592356688  46.706586826  0.0266527204  0.7110009193  7800          0.0826062489 
0.9932885906  0.9070904645  0.9968017058  0.9123931624  0.9992514970  0.9550898204  0.7627226463  0.7299363057  48.502994012  0.0229960464  0.7110009193  8100          0.0818651660 
0.9829164124  0.8973105134  0.9957356077  0.9358974359  0.9977544910  0.9401197605  0.7735368957  0.7490445860  50.299401197  0.0250121153  0.7110009193  8400          0.0826210308 
0.9975594875  0.9168704156  0.9914712154  0.9358974359  0.9992514970  0.9461077844  0.7757633588  0.7821656051  52.095808383  0.0353051164  0.7110009193  8700          0.0793306398 
0.9719341062  0.8728606357  0.9877398721  0.9230769231  0.9955089820  0.9311377246  0.7506361323  0.7401273885  53.892215568  0.0182909444  0.7110009193  9000          0.0792755135 
0.9932885906  0.8997555012  0.9968017058  0.9337606838  0.9955089820  0.9461077844  0.7423664122  0.7210191083  55.688622754  0.0242979378  0.7110009193  9300          0.0814352274 
0.9981696156  0.9046454768  0.9952025586  0.9081196581  0.9992514970  0.9550898204  0.7923027990  0.7859872611  57.485029940  0.0151348058  0.7110009193  9600          0.0821134520 
0.9847467968  0.9095354523  0.9941364606  0.9230769231  0.9977544910  0.9461077844  0.7493638677  0.7350318471  59.281437125  0.0224066464  0.7110009193  9900          0.0788835430 
0.9926784625  0.8973105134  0.9818763326  0.9401709402  0.9970059880  0.9550898204  0.7340966921  0.7312101911  59.874251497  0.0169415017  0.7110009193  9999          0.0779649079 
========== Best Results ==========
0.9981696156  0.9437652812  0.9968017058  0.9529914530  1.0000000000  0.9760479042  0.7923027990  0.7859872611  59.874251497  2.0157012939  0.7110009193  9999          0.9635336399 
Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: ERM
	alpha: None
	batch_size: 3
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: ERM
	pretrain: /mnt/disk1/nmduong/hust/m2cl/ERM/model_best_env3_out_acc.pkl
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: None
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 3
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: None
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Load checkpoint from /mnt/disk1/nmduong/hust/m2cl/ERM/model_best_env3_out_acc.pkl
0.9920683344  0.9242053790  0.9952025586  0.9401709402  0.9985029940  0.9371257485  0.7763994911  0.7859872611  0.4874377251 
Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: ERM
	alpha: None
	batch_size: 3
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: ERM
	pretrain: /mnt/disk1/nmduong/hust/m2cl/ERM/model_best_env3_out_acc.pkl
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: None
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 3
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: None
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Load checkpoint from /mnt/disk1/nmduong/hust/m2cl/ERM/model_best_env3_out_acc.pkl
0.9920683344  0.9242053790  0.9952025586  0.9401709402  0.9985029940  0.9371257485  0.7763994911  0.7859872611  0.4874377251 
