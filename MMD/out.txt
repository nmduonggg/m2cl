Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: MMD
	alpha: None
	batch_size: 8
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: MMD
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: 1.0
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	mmd_gamma: 1.0
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: 1.0
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        penalty       step          step_time    
0.1647345943  0.1784841076  0.1689765458  0.1495726496  0.2058383234  0.2035928144  0.1768447837  0.1732484076  0.0000000000  2.0157105923  0.6205182076  1.6806111336  0             1.3075325489 
0.9133618060  0.8557457213  0.8901918977  0.8504273504  0.9655688623  0.9311377246  0.6389949109  0.6471337580  1.7964071856  0.4590051734  0.6977558136  1.6615349905  300           0.1541651670 
0.9322757779  0.8777506112  0.9445628998  0.8867521368  0.9865269461  0.9431137725  0.7382315522  0.7464968153  3.5928143713  0.1919309381  0.6977558136  1.6214225050  600           0.1721343629 
0.9560707749  0.9193154034  0.9637526652  0.9209401709  0.9925149701  0.9491017964  0.7191475827  0.7388535032  5.3892215569  0.1474709432  0.6977558136  1.5834600107  900           0.1631960821 
0.9579011592  0.9022004890  0.9701492537  0.9294871795  0.9962574850  0.9670658683  0.7201017812  0.7439490446  7.1856287425  0.1176653015  0.6977558136  1.5566428479  1200          0.1661477470 
0.9646125686  0.8948655257  0.9760127932  0.9337606838  0.9940119760  0.9491017964  0.7585877863  0.7821656051  8.9820359281  0.1065797462  0.6977558136  1.5294985863  1500          0.1701352080 
0.9768151312  0.9095354523  0.9808102345  0.9294871795  0.9940119760  0.9580838323  0.7270992366  0.7248407643  10.778443113  0.1068840587  0.6977558136  1.5026423848  1800          0.1477872086 
0.9847467968  0.9193154034  0.9850746269  0.9273504274  0.9917664671  0.9550898204  0.7255089059  0.7363057325  12.574850299  0.0890417733  0.6977558136  1.4694796781  2100          0.1652481167 
0.9841366687  0.9266503667  0.9850746269  0.9358974359  0.9962574850  0.9550898204  0.6844783715  0.6738853503  14.371257485  0.0844690765  0.6977558136  1.4578596703  2400          0.1680831099 
0.9664429530  0.8899755501  0.9818763326  0.9017094017  0.9962574850  0.9550898204  0.7935750636  0.8050955414  16.167664670  0.0843211809  0.6977558136  1.4355308278  2700          0.1691331164 
0.9938987187  0.9070904645  0.9925373134  0.9465811966  0.9992514970  0.9550898204  0.7639949109  0.7757961783  17.964071856  0.0758958727  0.6977558136  1.4121155326  3000          0.1668149400 
0.9969493594  0.9193154034  0.9930703625  0.9401709402  0.9947604790  0.9401197605  0.7395038168  0.7363057325  19.760479041  0.0712310070  0.6977558136  1.4016792464  3300          0.1649988413 
0.9920683344  0.9339853301  0.9936034115  0.9059829060  0.9955089820  0.9491017964  0.7557251908  0.7477707006  21.556886227  0.0673172798  0.6977558136  1.3833636371  3600          0.1452626451 
0.9890176937  0.9144254279  0.9920042644  0.9358974359  0.9985029940  0.9700598802  0.7026081425  0.7082802548  23.353293413  0.0672389000  0.6977558136  1.3687355785  3900          0.1596126167 
0.9908480781  0.9193154034  0.9957356077  0.9423076923  0.9992514970  0.9580838323  0.7458651399  0.7464968153  25.149700598  0.0672212828  0.6977558136  1.3552112738  4200          0.1558225902 
0.9877974375  0.8875305623  0.9920042644  0.9252136752  0.9985029940  0.9520958084  0.7592239186  0.7656050955  26.946107784  0.0596823968  0.6977558136  1.3379627975  4500          0.0954239742 
0.9780353874  0.8826405868  0.9941364606  0.9358974359  0.9932634731  0.9520958084  0.7713104326  0.7745222930  28.742514970  0.0658266543  0.6977558136  1.3288140607  4800          0.0884034626 
0.9945088469  0.9266503667  0.9920042644  0.9273504274  1.0000000000  0.9520958084  0.7643129771  0.7503184713  30.538922155  0.0650074896  0.6977558136  1.3205313909  5100          0.0856606738 
0.9932885906  0.9119804401  0.9920042644  0.9252136752  0.9985029940  0.9520958084  0.7725826972  0.7656050955  32.335329341  0.0578787051  0.6977558136  1.3117431637  5400          0.0830075256 
0.9969493594  0.9290953545  0.9962686567  0.9423076923  1.0000000000  0.9700598802  0.7360050891  0.7515923567  34.131736526  0.0581394267  0.6977558136  1.2915958405  5700          0.0642988499 
0.9798657718  0.8997555012  0.9909381663  0.9230769231  0.9970059880  0.9341317365  0.7236005089  0.7146496815  35.928143712  0.0590412864  0.6977558136  1.2887564278  6000          0.0657156841 
0.9957291031  0.8997555012  0.9930703625  0.9252136752  0.9977544910  0.9520958084  0.7267811705  0.7171974522  37.724550898  0.0690586738  0.6977558136  1.2651243043  6300          0.0639414589 
0.9951189750  0.9193154034  0.9909381663  0.9102564103  1.0000000000  0.9520958084  0.7423664122  0.7426751592  39.520958083  0.0594483093  0.6977558136  1.2494563673  6600          0.0635084653 
0.9987797437  0.9095354523  0.9930703625  0.9401709402  1.0000000000  0.9520958084  0.7805343511  0.7630573248  41.317365269  0.0643019713  0.6977558136  1.2355044476  6900          0.0629316632 
0.9902379500  0.9193154034  0.9904051173  0.9252136752  0.9970059880  0.9610778443  0.7321882952  0.7464968153  43.113772455  0.0676244188  0.6977558136  1.2119848700  7200          0.0657311678 
0.9951189750  0.9046454768  0.9968017058  0.9444444444  0.9992514970  0.9640718563  0.7690839695  0.7719745223  44.910179640  0.0747987199  0.6977558136  1.1970921069  7500          0.0642243441 
0.9957291031  0.9070904645  0.9962686567  0.9465811966  0.9955089820  0.9520958084  0.7404580153  0.7401273885  46.706586826  0.0629512237  0.6977558136  1.1832661710  7800          0.0639468789 
0.9969493594  0.9046454768  0.9968017058  0.9273504274  1.0000000000  0.9580838323  0.7674936387  0.7770700637  48.502994012  0.0689687724  0.6977558136  1.1821257236  8100          0.0652926977 
0.9951189750  0.9144254279  0.9952025586  0.9209401709  0.9977544910  0.9550898204  0.7811704835  0.7898089172  50.299401197  0.0601877273  0.6977558136  1.1693435792  8400          0.0620208049 
0.9981696156  0.8875305623  0.9936034115  0.9465811966  1.0000000000  0.9461077844  0.7913486005  0.8050955414  52.095808383  0.0621948197  0.6977558136  1.1505177977  8700          0.0631929525 
0.9938987187  0.8973105134  0.9984008529  0.9508547009  0.9977544910  0.9580838323  0.7719465649  0.7834394904  53.892215568  0.0576321213  0.6977558136  1.1397776947  9000          0.0624979432 
0.9914582062  0.8948655257  0.9930703625  0.9166666667  0.9947604790  0.9431137725  0.7560432570  0.7554140127  55.688622754  0.0588879722  0.6977558136  1.1277255623  9300          0.0633994818 
0.9969493594  0.9193154034  0.9984008529  0.9444444444  1.0000000000  0.9461077844  0.7760814249  0.7834394904  57.485029940  0.0584724163  0.6977558136  1.1131042643  9600          0.0629874730 
0.9853569250  0.8753056235  0.9920042644  0.9316239316  0.9985029940  0.9520958084  0.7910305344  0.7745222930  59.281437125  0.0567278390  0.6977558136  1.1178610567  9900          0.0634115124 
0.9926784625  0.8875305623  0.9952025586  0.9380341880  1.0000000000  0.9550898204  0.7681297710  0.7668789809  59.874251497  0.0560603885  0.6977558136  1.1045367375  9999          0.0638471204 
========== Best Results ==========
0.9987797437  0.9339853301  0.9984008529  0.9508547009  1.0000000000  0.9700598802  0.7935750636  0.8050955414  59.874251497  2.0157105923  0.6977558136  1.6806111336  9999          1.3075325489 
Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: MMD
	alpha: None
	batch_size: 8
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: MMD
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: 1.0
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	mmd_gamma: 1.0
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: 1.0
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        penalty       step          step_time    
0.1598535692  0.1687041565  0.1908315565  0.1581196581  0.2080838323  0.2065868263  0.2127862595  0.2012738854  0.0000000000  1.7680497169  0.6205182076  1.7790340185  0             0.8831288815 
0.8560097621  0.8166259169  0.8470149254  0.8205128205  0.9064371257  0.8772455090  0.8606870229  0.8764331210  1.7964071856  0.6077112829  0.6977558136  1.6576571008  300           0.0635146022 
0.8798047590  0.8459657702  0.8869936034  0.8461538462  0.9528443114  0.9371257485  0.9090330789  0.8955414013  3.5928143713  0.2887811912  0.6977558136  1.6223504317  600           0.0625981418 
0.9444783405  0.9022004890  0.9536247335  0.9209401709  0.9176646707  0.8832335329  0.9398854962  0.9006369427  5.3892215569  0.2105724718  0.6977558136  1.6027802285  900           0.0630073659 
0.9585112874  0.8973105134  0.9642857143  0.9166666667  0.9468562874  0.9221556886  0.9541984733  0.9414012739  7.1856287425  0.1648141020  0.6977558136  1.5778543854  1200          0.0639679138 
0.9804758999  0.8973105134  0.9792110874  0.9316239316  0.9468562874  0.9461077844  0.9678753181  0.9222929936  8.9820359281  0.1427521662  0.6977558136  1.5553714403  1500          0.0638903220 
0.9823062843  0.9144254279  0.9754797441  0.9294871795  0.9348802395  0.8982035928  0.9720101781  0.9222929936  10.778443113  0.1209714715  0.6977558136  1.5316075977  1800          0.0628978475 
0.9884075656  0.9168704156  0.9872068230  0.9444444444  0.9333832335  0.9221556886  0.9697837150  0.9350318471  12.574850299  0.1125941292  0.6977558136  1.5111336577  2100          0.0626228992 
0.9896278218  0.9168704156  0.9904051173  0.9423076923  0.9498502994  0.9281437126  0.9697837150  0.9286624204  14.371257485  0.1043153167  0.6977558136  1.4911445137  2400          0.0637483613 
0.9938987187  0.9168704156  0.9904051173  0.9465811966  0.9528443114  0.9311377246  0.9726463104  0.9261146497  16.167664670  0.1103398702  0.6977558136  1.4704812026  2700          0.0641390936 
0.9926784625  0.9168704156  0.9920042644  0.9316239316  0.9483532934  0.9011976048  0.9793256997  0.9261146497  17.964071856  0.0884883310  0.6977558136  1.4510227203  3000          0.0635523216 
0.9877974375  0.9144254279  0.9893390192  0.9401709402  0.9423652695  0.9311377246  0.9831424936  0.9261146497  19.760479041  0.0840108345  0.6977558136  1.4362945644  3300          0.0621534244 
0.9914582062  0.9046454768  0.9888059701  0.9423076923  0.9401197605  0.9281437126  0.9872773537  0.9401273885  21.556886227  0.0919886946  0.6977558136  1.4224864336  3600          0.0635194039 
0.9865771812  0.9046454768  0.9957356077  0.9487179487  0.9483532934  0.9401197605  0.9895038168  0.9490445860  23.353293413  0.0778591654  0.6977558136  1.4008419879  3900          0.0630485710 
0.9920683344  0.9242053790  0.9957356077  0.9465811966  0.9468562874  0.9281437126  0.9888676845  0.9401273885  25.149700598  0.0729076276  0.6977558136  1.3922721152  4200          0.0653735439 
0.9945088469  0.8948655257  0.9973347548  0.9294871795  0.9326347305  0.9131736527  0.9933206107  0.9375796178  26.946107784  0.0764616727  0.6977558136  1.3785068623  4500          0.0635616215 
0.9945088469  0.9022004890  0.9952025586  0.9380341880  0.9341317365  0.9131736527  0.9875954198  0.9337579618  28.742514970  0.0682829402  0.6977558136  1.3734335879  4800          0.0637130125 
0.9914582062  0.9046454768  0.9952025586  0.9401709402  0.9348802395  0.9281437126  0.9844147583  0.9222929936  30.538922155  0.0666010095  0.6977558136  1.3557440416  5100          0.0616471386 
1.0000000000  0.9144254279  0.9946695096  0.9572649573  0.9438622754  0.9311377246  0.9882315522  0.9261146497  32.335329341  0.0608026822  0.6977558136  1.3456964115  5400          0.0624239580 
0.9981696156  0.9290953545  0.9941364606  0.9508547009  0.9356287425  0.9251497006  0.9898218830  0.9375796178  34.131736526  0.0644014980  0.6977558136  1.3376156759  5700          0.0612948815 
0.9932885906  0.9095354523  0.9978678038  0.9273504274  0.9431137725  0.9191616766  0.9898218830  0.9350318471  35.928143712  0.0611200910  0.6977558136  1.3218495083  6000          0.1023969905 
0.9945088469  0.9095354523  0.9989339019  0.9423076923  0.9378742515  0.9131736527  0.9898218830  0.9248407643  37.724550898  0.0739268293  0.6977558136  1.3173417163  6300          0.1124412529 
0.9938987187  0.9119804401  0.9957356077  0.9358974359  0.9431137725  0.9101796407  0.9939567430  0.9337579618  39.520958083  0.0606322860  0.6977558136  1.2933099922  6600          0.1158342584 
0.9951189750  0.9046454768  0.9957356077  0.9273504274  0.9356287425  0.9221556886  0.9910941476  0.9426751592  41.317365269  0.0675738697  0.6977558136  1.2869978956  6900          0.1067914724 
0.9938987187  0.8948655257  0.9978678038  0.9358974359  0.9288922156  0.9221556886  0.9914122137  0.9248407643  43.113772455  0.0680182597  0.6977558136  1.2796384219  7200          0.1211021407 
0.9890176937  0.8997555012  0.9978678038  0.9444444444  0.9476047904  0.9401197605  0.9930025445  0.9375796178  44.910179640  0.0689309864  0.6977558136  1.2573657537  7500          0.1059485491 
0.9908480781  0.8875305623  0.9952025586  0.9551282051  0.9363772455  0.9161676647  0.9885496183  0.9210191083  46.706586826  0.0725707314  0.6977558136  1.2408750264  7800          0.1176869710 
0.9957291031  0.9095354523  0.9973347548  0.9401709402  0.9438622754  0.9161676647  0.9968193384  0.9414012739  48.502994012  0.0701032423  0.6977558136  1.2217797554  8100          0.1096985324 
0.9951189750  0.8997555012  0.9978678038  0.9209401709  0.9154191617  0.9101796407  0.9885496183  0.9273885350  50.299401197  0.0704675459  0.6977558136  1.2103765694  8400          0.1169980780 
0.9951189750  0.8997555012  0.9946695096  0.9423076923  0.9371257485  0.9221556886  0.9945928753  0.9490445860  52.095808383  0.0727269094  0.6977558136  1.1920381939  8700          0.1106727266 
0.9981696156  0.9168704156  0.9962686567  0.9380341880  0.9408682635  0.9131736527  0.9936386768  0.9375796178  53.892215568  0.0700593452  0.6977558136  1.1867780968  9000          0.0998800699 
1.0000000000  0.9070904645  0.9994669510  0.9487179487  0.9483532934  0.9281437126  0.9955470738  0.9554140127  55.688622754  0.0678774376  0.6977558136  1.1830401788  9300          0.0996765383 
0.9780353874  0.8655256724  0.9824093817  0.9102564103  0.9071856287  0.9041916168  0.9742366412  0.9184713376  57.485029940  0.0620619926  0.6977558136  1.1584452126  9600          0.1183996177 
0.9975594875  0.9193154034  0.9973347548  0.9529914530  0.9401197605  0.9221556886  0.9907760814  0.9350318471  59.281437125  0.0626170653  0.6977558136  1.1567547794  9900          0.1136297186 
0.9926784625  0.8948655257  0.9973347548  0.9316239316  0.9416167665  0.9311377246  0.9895038168  0.9286624204  59.874251497  0.0630745070  0.6977558136  1.1529327721  9999          0.1203331201 
========== Best Results ==========
1.0000000000  0.9290953545  0.9994669510  0.9572649573  0.9528443114  0.9461077844  0.9968193384  0.9554140127  59.874251497  1.7680497169  0.6977558136  1.7790340185  9999          0.8831288815 
Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: MMD
	alpha: None
	batch_size: 8
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: MMD
	pretrain: /mnt/disk1/nmduong/hust/m2cl/MMD/model_best_env3_out_acc.pkl
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: None
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	mmd_gamma: 1.0
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: None
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Load checkpoint from /mnt/disk1/nmduong/hust/m2cl/MMD/model_best_env3_out_acc.pkl
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/mnt/disk1/nmduong/hust/m2cl/domainbed/scripts/test.py", line 249, in <module>
    feature = misc.tsne_collect(algorithm, loader, weights, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/nmduong/hust/m2cl/domainbed/lib/misc.py", line 224, in tsne_collect
    p, _, feature = network.predict(x, collect=True)
    ^^^^^^^^^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: MMD
	alpha: None
	batch_size: 8
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: MMD
	pretrain: /mnt/disk1/nmduong/hust/m2cl/MMD/model_best_env3_out_acc.pkl
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: None
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	mmd_gamma: 1.0
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: None
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Load checkpoint from /mnt/disk1/nmduong/hust/m2cl/MMD/model_best_env3_out_acc.pkl
0.9957291031  0.9217603912  0.9978678038  0.9508547009  0.9408682635  0.9011976048  0.9977735369  0.9528662420  0.4874377251 
Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: ERM
	alpha: None
	batch_size: 8
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: MMD
	pretrain: /mnt/disk1/nmduong/hust/m2cl/ERM/model_best_env3_out_acc.pkl
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: None
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: None
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/mnt/disk1/nmduong/hust/m2cl/domainbed/scripts/test.py", line 202, in <module>
    algorithm.load_state_dict(algorithm_dict['model_dict'])
  File "/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ERM:
	size mismatch for classifier.weight: copying a param with shape torch.Size([5, 512]) from checkpoint, the shape in current model is torch.Size([7, 512]).
	size mismatch for classifier.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([7]).
	size mismatch for network.1.weight: copying a param with shape torch.Size([5, 512]) from checkpoint, the shape in current model is torch.Size([7, 512]).
	size mismatch for network.1.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([7]).
Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: ERM
	alpha: None
	batch_size: 3
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: MMD
	pretrain: /mnt/disk1/nmduong/hust/m2cl/ERM/model_best_env3_out_acc.pkl
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: None
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 3
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: None
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Load checkpoint from /mnt/disk1/nmduong/hust/m2cl/ERM/model_best_env3_out_acc.pkl
0.9920683344  0.9242053790  0.9952025586  0.9401709402  0.9985029940  0.9371257485  0.7763994911  0.7859872611  0.4874377251 
Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: MMD
	alpha: None
	batch_size: 8
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: MMD
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: 1.0
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	mmd_gamma: 1.0
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: 1.0
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        penalty       step          step_time    
0.1598535692  0.1687041565  0.1908315565  0.1581196581  0.2080838323  0.2065868263  0.2127862595  0.2012738854  0.0000000000  1.7680497169  0.6205182076  1.7790340185  0             1.4527807236 
0.8560097621  0.8166259169  0.8470149254  0.8205128205  0.9064371257  0.8772455090  0.8606870229  0.8764331210  1.7964071856  0.6077112829  0.6977558136  1.6576571008  300           0.1230862292 
0.8798047590  0.8459657702  0.8869936034  0.8461538462  0.9528443114  0.9371257485  0.9090330789  0.8955414013  3.5928143713  0.2887811912  0.6977558136  1.6223504317  600           0.1315540441 
0.9444783405  0.9022004890  0.9536247335  0.9209401709  0.9176646707  0.8832335329  0.9398854962  0.9006369427  5.3892215569  0.2105724718  0.6977558136  1.6027802285  900           0.1261665312 
0.9585112874  0.8973105134  0.9642857143  0.9166666667  0.9468562874  0.9221556886  0.9541984733  0.9414012739  7.1856287425  0.1648141020  0.6977558136  1.5778543854  1200          0.1241289242 
0.9804758999  0.8973105134  0.9792110874  0.9316239316  0.9468562874  0.9461077844  0.9678753181  0.9222929936  8.9820359281  0.1427521662  0.6977558136  1.5553714403  1500          0.1301221045 
0.9823062843  0.9144254279  0.9754797441  0.9294871795  0.9348802395  0.8982035928  0.9720101781  0.9222929936  10.778443113  0.1209714715  0.6977558136  1.5316075977  1800          0.1253092337 
0.9884075656  0.9168704156  0.9872068230  0.9444444444  0.9333832335  0.9221556886  0.9697837150  0.9350318471  12.574850299  0.1125941292  0.6977558136  1.5111336577  2100          0.1323123868 
0.9896278218  0.9168704156  0.9904051173  0.9423076923  0.9498502994  0.9281437126  0.9697837150  0.9286624204  14.371257485  0.1043153167  0.6977558136  1.4911445137  2400          0.1241675393 
0.9938987187  0.9168704156  0.9904051173  0.9465811966  0.9528443114  0.9311377246  0.9726463104  0.9261146497  16.167664670  0.1103398702  0.6977558136  1.4704812026  2700          0.1291086491 
0.9926784625  0.9168704156  0.9920042644  0.9316239316  0.9483532934  0.9011976048  0.9793256997  0.9261146497  17.964071856  0.0884883310  0.6977558136  1.4510227203  3000          0.1249856337 
0.9877974375  0.9144254279  0.9893390192  0.9401709402  0.9423652695  0.9311377246  0.9831424936  0.9261146497  19.760479041  0.0840108345  0.6977558136  1.4362945644  3300          0.1250169841 
0.9914582062  0.9046454768  0.9888059701  0.9423076923  0.9401197605  0.9281437126  0.9872773537  0.9401273885  21.556886227  0.0919886946  0.6977558136  1.4224864336  3600          0.1277110394 
0.9865771812  0.9046454768  0.9957356077  0.9487179487  0.9483532934  0.9401197605  0.9895038168  0.9490445860  23.353293413  0.0778591654  0.6977558136  1.4008419879  3900          0.1225240787 
0.9920683344  0.9242053790  0.9957356077  0.9465811966  0.9468562874  0.9281437126  0.9888676845  0.9401273885  25.149700598  0.0729076276  0.6977558136  1.3922721152  4200          0.1288729922 
0.9945088469  0.8948655257  0.9973347548  0.9294871795  0.9326347305  0.9131736527  0.9933206107  0.9375796178  26.946107784  0.0764616727  0.6977558136  1.3785068623  4500          0.1262989505 
0.9945088469  0.9022004890  0.9952025586  0.9380341880  0.9341317365  0.9131736527  0.9875954198  0.9337579618  28.742514970  0.0682829402  0.6977558136  1.3734335879  4800          0.1284224073 
0.9914582062  0.9046454768  0.9952025586  0.9401709402  0.9348802395  0.9281437126  0.9844147583  0.9222929936  30.538922155  0.0666010095  0.6977558136  1.3557440416  5100          0.1297989162 
1.0000000000  0.9144254279  0.9946695096  0.9572649573  0.9438622754  0.9311377246  0.9882315522  0.9261146497  32.335329341  0.0608026822  0.6977558136  1.3456964115  5400          0.1302421459 
0.9981696156  0.9290953545  0.9941364606  0.9508547009  0.9356287425  0.9251497006  0.9898218830  0.9375796178  34.131736526  0.0644014980  0.6977558136  1.3376156759  5700          0.1372477055 
0.9932885906  0.9095354523  0.9978678038  0.9273504274  0.9431137725  0.9191616766  0.9898218830  0.9350318471  35.928143712  0.0611200910  0.6977558136  1.3218495083  6000          0.1304029338 
0.9945088469  0.9095354523  0.9989339019  0.9423076923  0.9378742515  0.9131736527  0.9898218830  0.9248407643  37.724550898  0.0739268293  0.6977558136  1.3173417163  6300          0.1429959226 
0.9938987187  0.9119804401  0.9957356077  0.9358974359  0.9431137725  0.9101796407  0.9939567430  0.9337579618  39.520958083  0.0606322860  0.6977558136  1.2933099922  6600          0.1308983032 
0.9951189750  0.9046454768  0.9957356077  0.9273504274  0.9356287425  0.9221556886  0.9910941476  0.9426751592  41.317365269  0.0675738697  0.6977558136  1.2869978956  6900          0.1276086791 
0.9938987187  0.8948655257  0.9978678038  0.9358974359  0.9288922156  0.9221556886  0.9914122137  0.9248407643  43.113772455  0.0680182597  0.6977558136  1.2796384219  7200          0.1190210319 
0.9890176937  0.8997555012  0.9978678038  0.9444444444  0.9476047904  0.9401197605  0.9930025445  0.9375796178  44.910179640  0.0689309864  0.6977558136  1.2573657537  7500          0.1250783261 
0.9908480781  0.8875305623  0.9952025586  0.9551282051  0.9363772455  0.9161676647  0.9885496183  0.9210191083  46.706586826  0.0725707314  0.6977558136  1.2408750264  7800          0.1311680992 
0.9957291031  0.9095354523  0.9973347548  0.9401709402  0.9438622754  0.9161676647  0.9968193384  0.9414012739  48.502994012  0.0701032423  0.6977558136  1.2217797554  8100          0.1266523806 
0.9951189750  0.8997555012  0.9978678038  0.9209401709  0.9154191617  0.9101796407  0.9885496183  0.9273885350  50.299401197  0.0704675459  0.6977558136  1.2103765694  8400          0.1364772789 
0.9951189750  0.8997555012  0.9946695096  0.9423076923  0.9371257485  0.9221556886  0.9945928753  0.9490445860  52.095808383  0.0727269094  0.6977558136  1.1920381939  8700          0.1259244617 
0.9981696156  0.9168704156  0.9962686567  0.9380341880  0.9408682635  0.9131736527  0.9936386768  0.9375796178  53.892215568  0.0700593452  0.6977558136  1.1867780968  9000          0.1246571795 
1.0000000000  0.9070904645  0.9994669510  0.9487179487  0.9483532934  0.9281437126  0.9955470738  0.9554140127  55.688622754  0.0678774376  0.6977558136  1.1830401788  9300          0.1247023567 
0.9780353874  0.8655256724  0.9824093817  0.9102564103  0.9071856287  0.9041916168  0.9742366412  0.9184713376  57.485029940  0.0620619926  0.6977558136  1.1584452126  9600          0.1217242344 
0.9975594875  0.9193154034  0.9973347548  0.9529914530  0.9401197605  0.9221556886  0.9907760814  0.9350318471  59.281437125  0.0626170653  0.6977558136  1.1567547794  9900          0.1322430698 
0.9926784625  0.8948655257  0.9973347548  0.9316239316  0.9416167665  0.9311377246  0.9895038168  0.9286624204  59.874251497  0.0630745070  0.6977558136  1.1529327721  9999          0.1235869148 
========== Best Results ==========
1.0000000000  0.9290953545  0.9994669510  0.9572649573  0.9528443114  0.9461077844  0.9968193384  0.9554140127  59.874251497  1.7680497169  0.6977558136  1.7790340185  9999          1.4527807236 
Environment:
	Python: 3.11.8
	PyTorch: 2.2.2+cu121
	Torchvision: 0.17.2+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.26.4
	PIL: 10.3.0
Args:
	algorithm: MMD
	alpha: None
	batch_size: 8
	beta: None
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	lparam: None
	lr: None
	output_dir: MMD
	pretrain: /mnt/disk1/nmduong/hust/m2cl/MMD/model_best_env3_out_acc.pkl
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: 10000
	task: domain_generalization
	temp: None
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: True
	lparam: None
	lr: 5e-05
	mmd_gamma: 1.0
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	temp: None
	weight_decay: 0.0
Initialize ResNet18
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mnt/disk1/anaconda3/envs/nmduong/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Load checkpoint from /mnt/disk1/nmduong/hust/m2cl/MMD/model_best_env3_out_acc.pkl
0.9957291031  0.9217603912  0.9978678038  0.9508547009  0.9408682635  0.9011976048  0.9977735369  0.9528662420  0.4874377251 
